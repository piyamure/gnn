{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch_geometric.datasets import QM9\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph convolution layer (this performs two convolutions)\n",
    "class BasicGraphConvolutionLayer(torch.nn.Module):\n",
    "    def __init__(self,in_chanels,out_channels):\n",
    "        super().__init__()\n",
    "        self.in_chanels = in_chanels\n",
    "        self.out_chanels = out_channels\n",
    "        self.w1 = Parameter(torch.rand(in_chanels,out_channels,dtype=torch.float32))\n",
    "        self.w2 = Parameter(torch.rand(in_chanels,out_channels,dtype=torch.float32))\n",
    "        self.bias = Parameter(torch.zeros(out_channels,dtype=torch.float32))\n",
    "        \n",
    "    def forward(self,X,A):\n",
    "        potential_msgs = torch.mm(X,self.w2)\n",
    "        propergated_msgs = torch.mm(A,potential_msgs)\n",
    "        root_update = torch.mm(X,self.w1)\n",
    "        output = propergated_msgs + root_update + self.bias \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum graph features: one feature mat for each graph (global pooling layer)\n",
    "def global_sum_pool(X,batch_mat):\n",
    "    if batch_mat is None or batch_mat.dim() == 1:\n",
    "        return torch.sum(X,dim=0).unsqueeze(dim=0)\n",
    "    else:\n",
    "        return torch.mm(batch_mat,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fucntion takes the size of graphs in a btach and return graph selection mask\n",
    "# using this graph selection mask, you can add the features of each graphs together \n",
    "def get_batch_tensor(graph_sizes):\n",
    "    starts= [sum(graph_sizes[:idx]) for idx in range(len(graph_sizes))]\n",
    "    stops = [starts[idx] + graph_sizes[idx] for idx in range(len(graph_sizes))]\n",
    "    total_len = sum(graph_sizes)\n",
    "    batch_size =len(graph_sizes)\n",
    "    batch_mat = torch.zeros([batch_size,total_len])\n",
    "    \n",
    "    for idx,starts_and_stops in enumerate(zip(starts,stops)):\n",
    "        start = starts_and_stops[0]\n",
    "        stop = starts_and_stops[1]\n",
    "        batch_mat[idx,start:stop] = 1\n",
    "    return batch_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_grap(batch):\n",
    "    adj_mats = [graph['A'] for graph in batch]\n",
    "    sizes = [A.size(0) for A in adj_mats]\n",
    "    tot_size = sum(sizes)\n",
    "    batch_mat = get_batch_tensor(sizes)\n",
    "    feat_mats = torch.cat([graph['X'] for graph in batch],dim=0)\n",
    "    labels = torch.cat([graph['y'] for graph in batch],dim=0)\n",
    "    batch_adj = torch.zeros([tot_size,tot_size],dtype=torch.float32)\n",
    "    accum = 0 \n",
    "    for adj in adj_mats:\n",
    "        g_size = adj.shape[0]\n",
    "        batch_adj[accum:accum + g_size,accum:accum + g_size] = adj\n",
    "        accum = accum + g_size\n",
    "    repr_and_label = {'A':batch_adj,'X':feat_mats,'y':labels,'batch':batch_mat}\n",
    "    return repr_and_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeNetwork(torch.nn.Module):\n",
    "    def __init__(self,input_features):\n",
    "        super().__init__()\n",
    "        self.conv_1 = BasicGraphConvolutionLayer(input_features,32)\n",
    "        self.conv_2 = BasicGraphConvolutionLayer(32,32)\n",
    "        self.fc_1 = torch.nn.Linear(32,16)\n",
    "        self.out_layer = torch.nn.Linear(16,2)\n",
    "        \n",
    "    def forwrd(self,X,A,batch_mat):\n",
    "        x = F.relu(self.conv_1(X,A))\n",
    "        x = F.relu(self.conv_2(x,A))\n",
    "        output = global_sum_pool(x,batch_mat)\n",
    "        output = self.fc_1(output)\n",
    "        output = self.out_layer(output)\n",
    "        return F.softmax(output,dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
